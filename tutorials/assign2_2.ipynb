{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step1 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: gym in /Users/HuangKan/Library/Python/3.7/lib/python/site-packages (0.17.2)\nRequirement already satisfied: scipy in /Users/HuangKan/Library/Python/3.7/lib/python/site-packages (from gym) (1.3.1)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /Users/HuangKan/Library/Python/3.7/lib/python/site-packages (from gym) (1.5.0)\nRequirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /Users/HuangKan/Library/Python/3.7/lib/python/site-packages (from gym) (1.2.1)\nRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/site-packages (from gym) (1.16.4)\nRequirement already satisfied: future in /usr/local/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.17.1)\n"
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step2 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step3 Agent\n",
    "* `Agent`是和环境`environment`交互的主体。\n",
    "* `predict()`方法：输入观察值`observation`（或者说状态`state`），输出动作值\n",
    "* `sample()`方法：在`predict()`方法基础上使用`ε-greedy`增加探索\n",
    "* `learn()`方法：输入训练数据，完成一轮Q表格的更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearningAgent(object):\n",
    "    def __init__(self, obs_n, act_n, learning_rate=0.01, gamma=0.9, e_greed=0.1):\n",
    "        self.act_n = act_n      # 动作维度，有几个动作可选\n",
    "        self.lr = learning_rate # 学习率\n",
    "        self.gamma = gamma      # reward的衰减率\n",
    "        self.epsilon = e_greed  # 按一定概率随机选动作\n",
    "        self.Q = np.zeros((obs_n, act_n))\n",
    "\n",
    "    # 根据输入观察值，采样输出的动作值，带探索\n",
    "    def sample(self, obs):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 1. 请完成sample函数功能\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        if np.random.uniform(0, 1) < (1.0 - self.epsilon): # 概率下根据Q值选择动作\n",
    "            action = self.predict(obs)\n",
    "        else: # epsilon 概率下随机选择动作\n",
    "            action = np.random.choice(self.act_n)\n",
    "        return action\n",
    "\n",
    "    # 根据输入观察值，预测输出的动作值\n",
    "    def predict(self, obs):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 2. 请完成predict函数功能\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        Q_list = self.Q[obs, :]\n",
    "        maxQ = np.max(Q_list)\n",
    "        action_list = np.where(Q_list == maxQ)[0]\n",
    "        action = np.random.choice(action_list)\n",
    "        return action\n",
    "\n",
    "    # 学习方法，也就是更新Q-table的方法\n",
    "    def learn(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\" off-policy\n",
    "            obs: 交互前的obs, s_t\n",
    "            action: 本次交互选择的action, a_t\n",
    "            reward: 本次动作获得的奖励r\n",
    "            next_obs: 本次交互后的obs, s_t+1\n",
    "            done: episode是否结束\n",
    "        \"\"\"\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 3. 请完成learn函数功能（Q-learning）\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        predict_Q = self.Q[obs, action]\n",
    "        if done:\n",
    "            target_Q = reward\n",
    "        else:\n",
    "            target_Q = reward + self.gamma * np.max(self.Q[next_obs, :])\n",
    "        self.Q[obs, action] += self.lr * (target_Q - predict_Q)\n",
    "\n",
    "    # 保存Q表格数据到文件\n",
    "    def save(self):\n",
    "        npy_file = './q_table.npy'\n",
    "        np.save(npy_file, self.Q)\n",
    "        print(npy_file + ' saved.')\n",
    "    \n",
    "    # 从文件中读取数据到Q表格中\n",
    "    def restore(self, npy_file='./q_table.npy'):\n",
    "        self.Q = np.load(npy_file)\n",
    "        print(npy_file + ' loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step4 Training && Test（训练&&测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_episode(env, agent, render=False):\n",
    "    total_steps = 0 # 记录每个episode走了多少step\n",
    "    total_reward = 0\n",
    "\n",
    "    obs = env.reset() # 重置环境, 重新开一局（即开始新的一个episode）\n",
    "\n",
    "    while True:\n",
    "        action = agent.sample(obs) # 根据算法选择一个动作\n",
    "        next_obs, reward, done, _ = env.step(action) # 与环境进行一个交互\n",
    "        # 训练 Q-learning算法\n",
    "        agent.learn(obs, action, reward, next_obs, done)\n",
    "\n",
    "        obs = next_obs  # 存储上一个观察值\n",
    "        total_reward += reward\n",
    "        total_steps += 1 # 计算step数\n",
    "        if render:\n",
    "            env.render() #渲染新的一帧图形\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, total_steps\n",
    "\n",
    "def test_episode(env, agent):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action = agent.predict(obs) # greedy\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        obs = next_obs\n",
    "        # time.sleep(0.5)\n",
    "        # env.render()\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step5 创建环境和Agent，启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Episode 0: steps = 14 , reward = 0.0\nEpisode 1: steps = 20 , reward = 0.0\nEpisode 2: steps = 2 , reward = 0.0\nEpisode 3: steps = 2 , reward = 0.0\nEpisode 4: steps = 2 , reward = 0.0\nEpisode 5: steps = 21 , reward = 0.0\nEpisode 6: steps = 11 , reward = 0.0\nEpisode 7: steps = 5 , reward = 0.0\nEpisode 8: steps = 9 , reward = 0.0\nEpisode 9: steps = 2 , reward = 0.0\nEpisode 10: steps = 20 , reward = 0.0\nEpisode 11: steps = 3 , reward = 0.0\nEpisode 12: steps = 3 , reward = 0.0\nEpisode 13: steps = 7 , reward = 0.0\nEpisode 14: steps = 8 , reward = 0.0\nEpisode 15: steps = 2 , reward = 0.0\nEpisode 16: steps = 6 , reward = 0.0\nEpisode 17: steps = 11 , reward = 0.0\nEpisode 18: steps = 8 , reward = 0.0\nEpisode 19: steps = 7 , reward = 0.0\nEpisode 20: steps = 3 , reward = 0.0\nEpisode 21: steps = 2 , reward = 0.0\nEpisode 22: steps = 4 , reward = 0.0\nEpisode 23: steps = 6 , reward = 0.0\nEpisode 24: steps = 12 , reward = 0.0\nEpisode 25: steps = 3 , reward = 0.0\nEpisode 26: steps = 3 , reward = 0.0\nEpisode 27: steps = 4 , reward = 0.0\nEpisode 28: steps = 3 , reward = 0.0\nEpisode 29: steps = 4 , reward = 0.0\nEpisode 30: steps = 8 , reward = 0.0\nEpisode 31: steps = 5 , reward = 0.0\nEpisode 32: steps = 19 , reward = 0.0\nEpisode 33: steps = 8 , reward = 0.0\nEpisode 34: steps = 3 , reward = 0.0\nEpisode 35: steps = 11 , reward = 0.0\nEpisode 36: steps = 2 , reward = 0.0\nEpisode 37: steps = 3 , reward = 0.0\nEpisode 38: steps = 5 , reward = 0.0\nEpisode 39: steps = 6 , reward = 0.0\nEpisode 40: steps = 9 , reward = 0.0\nEpisode 41: steps = 20 , reward = 0.0\nEpisode 42: steps = 10 , reward = 0.0\nEpisode 43: steps = 10 , reward = 0.0\nEpisode 44: steps = 2 , reward = 0.0\nEpisode 45: steps = 8 , reward = 0.0\nEpisode 46: steps = 7 , reward = 0.0\nEpisode 47: steps = 10 , reward = 0.0\nEpisode 48: steps = 9 , reward = 0.0\nEpisode 49: steps = 5 , reward = 0.0\nEpisode 50: steps = 2 , reward = 0.0\nEpisode 51: steps = 2 , reward = 0.0\nEpisode 52: steps = 8 , reward = 0.0\nEpisode 53: steps = 5 , reward = 0.0\nEpisode 54: steps = 3 , reward = 0.0\nEpisode 55: steps = 2 , reward = 0.0\nEpisode 56: steps = 2 , reward = 0.0\nEpisode 57: steps = 5 , reward = 0.0\nEpisode 58: steps = 3 , reward = 0.0\nEpisode 59: steps = 6 , reward = 0.0\nEpisode 60: steps = 5 , reward = 0.0\nEpisode 61: steps = 2 , reward = 0.0\nEpisode 62: steps = 12 , reward = 0.0\nEpisode 63: steps = 6 , reward = 0.0\nEpisode 64: steps = 6 , reward = 0.0\nEpisode 65: steps = 5 , reward = 0.0\nEpisode 66: steps = 7 , reward = 0.0\nEpisode 67: steps = 5 , reward = 0.0\nEpisode 68: steps = 4 , reward = 0.0\nEpisode 69: steps = 4 , reward = 0.0\nEpisode 70: steps = 5 , reward = 0.0\nEpisode 71: steps = 8 , reward = 0.0\nEpisode 72: steps = 6 , reward = 0.0\nEpisode 73: steps = 8 , reward = 0.0\nEpisode 74: steps = 2 , reward = 0.0\nEpisode 75: steps = 6 , reward = 0.0\nEpisode 76: steps = 18 , reward = 0.0\nEpisode 77: steps = 13 , reward = 1.0\nEpisode 78: steps = 14 , reward = 0.0\nEpisode 79: steps = 19 , reward = 0.0\nEpisode 80: steps = 15 , reward = 0.0\nEpisode 81: steps = 13 , reward = 0.0\nEpisode 82: steps = 7 , reward = 0.0\nEpisode 83: steps = 16 , reward = 0.0\nEpisode 84: steps = 2 , reward = 0.0\nEpisode 85: steps = 4 , reward = 0.0\nEpisode 86: steps = 3 , reward = 0.0\nEpisode 87: steps = 2 , reward = 0.0\nEpisode 88: steps = 10 , reward = 0.0\nEpisode 89: steps = 4 , reward = 0.0\nEpisode 90: steps = 4 , reward = 0.0\nEpisode 91: steps = 10 , reward = 0.0\nEpisode 92: steps = 14 , reward = 0.0\nEpisode 93: steps = 2 , reward = 0.0\nEpisode 94: steps = 3 , reward = 0.0\nEpisode 95: steps = 14 , reward = 0.0\nEpisode 96: steps = 8 , reward = 0.0\nEpisode 97: steps = 2 , reward = 0.0\nEpisode 98: steps = 9 , reward = 0.0\nEpisode 99: steps = 3 , reward = 0.0\nEpisode 100: steps = 5 , reward = 0.0\nEpisode 101: steps = 4 , reward = 0.0\nEpisode 102: steps = 7 , reward = 0.0\nEpisode 103: steps = 12 , reward = 0.0\nEpisode 104: steps = 12 , reward = 0.0\nEpisode 105: steps = 19 , reward = 0.0\nEpisode 106: steps = 2 , reward = 0.0\nEpisode 107: steps = 3 , reward = 0.0\nEpisode 108: steps = 5 , reward = 0.0\nEpisode 109: steps = 6 , reward = 0.0\nEpisode 110: steps = 2 , reward = 0.0\nEpisode 111: steps = 2 , reward = 0.0\nEpisode 112: steps = 6 , reward = 0.0\nEpisode 113: steps = 6 , reward = 0.0\nEpisode 114: steps = 6 , reward = 0.0\nEpisode 115: steps = 11 , reward = 0.0\nEpisode 116: steps = 14 , reward = 1.0\nEpisode 117: steps = 17 , reward = 1.0\nEpisode 118: steps = 16 , reward = 1.0\nEpisode 119: steps = 6 , reward = 1.0\nEpisode 120: steps = 18 , reward = 0.0\nEpisode 121: steps = 8 , reward = 1.0\nEpisode 122: steps = 5 , reward = 0.0\nEpisode 123: steps = 7 , reward = 1.0\nEpisode 124: steps = 6 , reward = 1.0\nEpisode 125: steps = 8 , reward = 0.0\nEpisode 126: steps = 6 , reward = 1.0\nEpisode 127: steps = 6 , reward = 1.0\nEpisode 128: steps = 6 , reward = 1.0\nEpisode 129: steps = 6 , reward = 1.0\nEpisode 130: steps = 6 , reward = 1.0\nEpisode 131: steps = 9 , reward = 1.0\nEpisode 132: steps = 7 , reward = 1.0\nEpisode 133: steps = 6 , reward = 1.0\nEpisode 134: steps = 6 , reward = 1.0\nEpisode 135: steps = 6 , reward = 1.0\nEpisode 136: steps = 6 , reward = 1.0\nEpisode 137: steps = 6 , reward = 1.0\nEpisode 138: steps = 8 , reward = 1.0\nEpisode 139: steps = 6 , reward = 1.0\nEpisode 140: steps = 6 , reward = 1.0\nEpisode 141: steps = 6 , reward = 1.0\nEpisode 142: steps = 8 , reward = 1.0\nEpisode 143: steps = 6 , reward = 1.0\nEpisode 144: steps = 6 , reward = 1.0\nEpisode 145: steps = 6 , reward = 1.0\nEpisode 146: steps = 2 , reward = 0.0\nEpisode 147: steps = 6 , reward = 1.0\nEpisode 148: steps = 8 , reward = 1.0\nEpisode 149: steps = 6 , reward = 1.0\nEpisode 150: steps = 6 , reward = 1.0\nEpisode 151: steps = 6 , reward = 1.0\nEpisode 152: steps = 6 , reward = 1.0\nEpisode 153: steps = 6 , reward = 1.0\nEpisode 154: steps = 6 , reward = 1.0\nEpisode 155: steps = 4 , reward = 0.0\nEpisode 156: steps = 7 , reward = 1.0\nEpisode 157: steps = 5 , reward = 0.0\nEpisode 158: steps = 10 , reward = 1.0\nEpisode 159: steps = 2 , reward = 0.0\nEpisode 160: steps = 6 , reward = 1.0\nEpisode 161: steps = 6 , reward = 1.0\nEpisode 162: steps = 6 , reward = 1.0\nEpisode 163: steps = 6 , reward = 1.0\nEpisode 164: steps = 6 , reward = 1.0\nEpisode 165: steps = 6 , reward = 1.0\nEpisode 166: steps = 6 , reward = 1.0\nEpisode 167: steps = 8 , reward = 1.0\nEpisode 168: steps = 7 , reward = 1.0\nEpisode 169: steps = 6 , reward = 1.0\nEpisode 170: steps = 6 , reward = 1.0\nEpisode 171: steps = 6 , reward = 1.0\nEpisode 172: steps = 6 , reward = 1.0\nEpisode 173: steps = 6 , reward = 1.0\nEpisode 174: steps = 6 , reward = 1.0\nEpisode 175: steps = 7 , reward = 1.0\nEpisode 176: steps = 6 , reward = 1.0\nEpisode 177: steps = 6 , reward = 1.0\nEpisode 178: steps = 7 , reward = 1.0\nEpisode 179: steps = 6 , reward = 1.0\nEpisode 180: steps = 6 , reward = 1.0\nEpisode 181: steps = 6 , reward = 1.0\nEpisode 182: steps = 10 , reward = 1.0\nEpisode 183: steps = 3 , reward = 0.0\nEpisode 184: steps = 6 , reward = 1.0\nEpisode 185: steps = 10 , reward = 1.0\nEpisode 186: steps = 7 , reward = 1.0\nEpisode 187: steps = 8 , reward = 1.0\nEpisode 188: steps = 6 , reward = 1.0\nEpisode 189: steps = 5 , reward = 0.0\nEpisode 190: steps = 6 , reward = 1.0\nEpisode 191: steps = 10 , reward = 1.0\nEpisode 192: steps = 7 , reward = 0.0\nEpisode 193: steps = 6 , reward = 1.0\nEpisode 194: steps = 2 , reward = 0.0\nEpisode 195: steps = 7 , reward = 1.0\nEpisode 196: steps = 9 , reward = 1.0\nEpisode 197: steps = 6 , reward = 1.0\nEpisode 198: steps = 6 , reward = 1.0\nEpisode 199: steps = 7 , reward = 1.0\nEpisode 200: steps = 6 , reward = 1.0\nEpisode 201: steps = 8 , reward = 1.0\nEpisode 202: steps = 6 , reward = 1.0\nEpisode 203: steps = 6 , reward = 1.0\nEpisode 204: steps = 3 , reward = 0.0\nEpisode 205: steps = 6 , reward = 1.0\nEpisode 206: steps = 6 , reward = 1.0\nEpisode 207: steps = 6 , reward = 1.0\nEpisode 208: steps = 3 , reward = 0.0\nEpisode 209: steps = 4 , reward = 0.0\nEpisode 210: steps = 6 , reward = 1.0\nEpisode 211: steps = 6 , reward = 1.0\nEpisode 212: steps = 6 , reward = 1.0\nEpisode 213: steps = 6 , reward = 1.0\nEpisode 214: steps = 5 , reward = 0.0\nEpisode 215: steps = 6 , reward = 1.0\nEpisode 216: steps = 7 , reward = 1.0\nEpisode 217: steps = 6 , reward = 1.0\nEpisode 218: steps = 7 , reward = 1.0\nEpisode 219: steps = 7 , reward = 1.0\nEpisode 220: steps = 6 , reward = 1.0\nEpisode 221: steps = 6 , reward = 1.0\nEpisode 222: steps = 4 , reward = 0.0\nEpisode 223: steps = 6 , reward = 1.0\nEpisode 224: steps = 6 , reward = 1.0\nEpisode 225: steps = 8 , reward = 1.0\nEpisode 226: steps = 5 , reward = 0.0\nEpisode 227: steps = 12 , reward = 1.0\nEpisode 228: steps = 6 , reward = 1.0\nEpisode 229: steps = 6 , reward = 1.0\nEpisode 230: steps = 6 , reward = 1.0\nEpisode 231: steps = 7 , reward = 1.0\nEpisode 232: steps = 6 , reward = 1.0\nEpisode 233: steps = 6 , reward = 1.0\nEpisode 234: steps = 8 , reward = 1.0\nEpisode 235: steps = 6 , reward = 1.0\nEpisode 236: steps = 8 , reward = 1.0\nEpisode 237: steps = 6 , reward = 1.0\nEpisode 238: steps = 6 , reward = 1.0\nEpisode 239: steps = 8 , reward = 1.0\nEpisode 240: steps = 6 , reward = 1.0\nEpisode 241: steps = 6 , reward = 1.0\nEpisode 242: steps = 6 , reward = 1.0\nEpisode 243: steps = 5 , reward = 0.0\nEpisode 244: steps = 7 , reward = 1.0\nEpisode 245: steps = 6 , reward = 1.0\nEpisode 246: steps = 8 , reward = 1.0\nEpisode 247: steps = 6 , reward = 1.0\nEpisode 248: steps = 6 , reward = 1.0\nEpisode 249: steps = 10 , reward = 1.0\nEpisode 250: steps = 6 , reward = 1.0\nEpisode 251: steps = 8 , reward = 1.0\nEpisode 252: steps = 8 , reward = 1.0\nEpisode 253: steps = 7 , reward = 1.0\nEpisode 254: steps = 6 , reward = 1.0\nEpisode 255: steps = 6 , reward = 1.0\nEpisode 256: steps = 9 , reward = 1.0\nEpisode 257: steps = 6 , reward = 1.0\nEpisode 258: steps = 9 , reward = 1.0\nEpisode 259: steps = 6 , reward = 1.0\nEpisode 260: steps = 6 , reward = 1.0\nEpisode 261: steps = 6 , reward = 1.0\nEpisode 262: steps = 6 , reward = 1.0\nEpisode 263: steps = 6 , reward = 1.0\nEpisode 264: steps = 7 , reward = 1.0\nEpisode 265: steps = 6 , reward = 1.0\nEpisode 266: steps = 8 , reward = 1.0\nEpisode 267: steps = 6 , reward = 1.0\nEpisode 268: steps = 6 , reward = 1.0\nEpisode 269: steps = 6 , reward = 1.0\nEpisode 270: steps = 6 , reward = 1.0\nEpisode 271: steps = 6 , reward = 1.0\nEpisode 272: steps = 10 , reward = 1.0\nEpisode 273: steps = 8 , reward = 1.0\nEpisode 274: steps = 10 , reward = 1.0\nEpisode 275: steps = 7 , reward = 1.0\nEpisode 276: steps = 6 , reward = 1.0\nEpisode 277: steps = 9 , reward = 1.0\nEpisode 278: steps = 6 , reward = 1.0\nEpisode 279: steps = 6 , reward = 1.0\nEpisode 280: steps = 6 , reward = 1.0\nEpisode 281: steps = 6 , reward = 1.0\nEpisode 282: steps = 6 , reward = 1.0\nEpisode 283: steps = 9 , reward = 1.0\nEpisode 284: steps = 6 , reward = 1.0\nEpisode 285: steps = 6 , reward = 1.0\nEpisode 286: steps = 3 , reward = 0.0\nEpisode 287: steps = 6 , reward = 1.0\nEpisode 288: steps = 6 , reward = 1.0\nEpisode 289: steps = 6 , reward = 1.0\nEpisode 290: steps = 6 , reward = 1.0\nEpisode 291: steps = 8 , reward = 1.0\nEpisode 292: steps = 6 , reward = 1.0\nEpisode 293: steps = 2 , reward = 0.0\nEpisode 294: steps = 8 , reward = 1.0\nEpisode 295: steps = 6 , reward = 1.0\nEpisode 296: steps = 6 , reward = 1.0\nEpisode 297: steps = 6 , reward = 1.0\nEpisode 298: steps = 7 , reward = 1.0\nEpisode 299: steps = 4 , reward = 0.0\nEpisode 300: steps = 6 , reward = 1.0\nEpisode 301: steps = 7 , reward = 1.0\nEpisode 302: steps = 10 , reward = 1.0\nEpisode 303: steps = 8 , reward = 1.0\nEpisode 304: steps = 6 , reward = 1.0\nEpisode 305: steps = 6 , reward = 1.0\nEpisode 306: steps = 4 , reward = 0.0\nEpisode 307: steps = 6 , reward = 1.0\nEpisode 308: steps = 6 , reward = 1.0\nEpisode 309: steps = 6 , reward = 1.0\nEpisode 310: steps = 13 , reward = 1.0\nEpisode 311: steps = 6 , reward = 1.0\nEpisode 312: steps = 6 , reward = 1.0\nEpisode 313: steps = 6 , reward = 1.0\nEpisode 314: steps = 6 , reward = 1.0\nEpisode 315: steps = 6 , reward = 1.0\nEpisode 316: steps = 6 , reward = 1.0\nEpisode 317: steps = 6 , reward = 1.0\nEpisode 318: steps = 6 , reward = 1.0\nEpisode 319: steps = 6 , reward = 1.0\nEpisode 320: steps = 8 , reward = 1.0\nEpisode 321: steps = 6 , reward = 1.0\nEpisode 322: steps = 4 , reward = 0.0\nEpisode 323: steps = 6 , reward = 1.0\nEpisode 324: steps = 6 , reward = 1.0\nEpisode 325: steps = 6 , reward = 1.0\nEpisode 326: steps = 6 , reward = 1.0\nEpisode 327: steps = 3 , reward = 0.0\nEpisode 328: steps = 6 , reward = 1.0\nEpisode 329: steps = 6 , reward = 1.0\nEpisode 330: steps = 6 , reward = 1.0\nEpisode 331: steps = 10 , reward = 1.0\nEpisode 332: steps = 6 , reward = 1.0\nEpisode 333: steps = 4 , reward = 0.0\nEpisode 334: steps = 6 , reward = 1.0\nEpisode 335: steps = 3 , reward = 0.0\nEpisode 336: steps = 7 , reward = 1.0\nEpisode 337: steps = 7 , reward = 1.0\nEpisode 338: steps = 8 , reward = 1.0\nEpisode 339: steps = 7 , reward = 1.0\nEpisode 340: steps = 6 , reward = 1.0\nEpisode 341: steps = 6 , reward = 1.0\nEpisode 342: steps = 6 , reward = 1.0\nEpisode 343: steps = 6 , reward = 1.0\nEpisode 344: steps = 14 , reward = 1.0\nEpisode 345: steps = 6 , reward = 1.0\nEpisode 346: steps = 7 , reward = 1.0\nEpisode 347: steps = 6 , reward = 1.0\nEpisode 348: steps = 4 , reward = 0.0\nEpisode 349: steps = 7 , reward = 1.0\nEpisode 350: steps = 6 , reward = 1.0\nEpisode 351: steps = 6 , reward = 1.0\nEpisode 352: steps = 6 , reward = 1.0\nEpisode 353: steps = 6 , reward = 1.0\nEpisode 354: steps = 6 , reward = 1.0\nEpisode 355: steps = 6 , reward = 1.0\nEpisode 356: steps = 6 , reward = 1.0\nEpisode 357: steps = 6 , reward = 1.0\nEpisode 358: steps = 6 , reward = 1.0\nEpisode 359: steps = 7 , reward = 1.0\nEpisode 360: steps = 3 , reward = 0.0\nEpisode 361: steps = 12 , reward = 1.0\nEpisode 362: steps = 8 , reward = 1.0\nEpisode 363: steps = 7 , reward = 1.0\nEpisode 364: steps = 6 , reward = 1.0\nEpisode 365: steps = 7 , reward = 1.0\nEpisode 366: steps = 6 , reward = 1.0\nEpisode 367: steps = 6 , reward = 1.0\nEpisode 368: steps = 6 , reward = 1.0\nEpisode 369: steps = 6 , reward = 1.0\nEpisode 370: steps = 6 , reward = 1.0\nEpisode 371: steps = 6 , reward = 1.0\nEpisode 372: steps = 2 , reward = 0.0\nEpisode 373: steps = 12 , reward = 1.0\nEpisode 374: steps = 7 , reward = 1.0\nEpisode 375: steps = 6 , reward = 1.0\nEpisode 376: steps = 7 , reward = 1.0\nEpisode 377: steps = 8 , reward = 1.0\nEpisode 378: steps = 6 , reward = 1.0\nEpisode 379: steps = 5 , reward = 0.0\nEpisode 380: steps = 6 , reward = 1.0\nEpisode 381: steps = 9 , reward = 1.0\nEpisode 382: steps = 6 , reward = 1.0\nEpisode 383: steps = 6 , reward = 1.0\nEpisode 384: steps = 8 , reward = 1.0\nEpisode 385: steps = 6 , reward = 1.0\nEpisode 386: steps = 6 , reward = 1.0\nEpisode 387: steps = 6 , reward = 1.0\nEpisode 388: steps = 6 , reward = 1.0\nEpisode 389: steps = 8 , reward = 1.0\nEpisode 390: steps = 6 , reward = 1.0\nEpisode 391: steps = 6 , reward = 1.0\nEpisode 392: steps = 6 , reward = 1.0\nEpisode 393: steps = 6 , reward = 1.0\nEpisode 394: steps = 6 , reward = 1.0\nEpisode 395: steps = 6 , reward = 1.0\nEpisode 396: steps = 6 , reward = 1.0\nEpisode 397: steps = 5 , reward = 0.0\nEpisode 398: steps = 7 , reward = 1.0\nEpisode 399: steps = 6 , reward = 1.0\nEpisode 400: steps = 7 , reward = 1.0\nEpisode 401: steps = 6 , reward = 1.0\nEpisode 402: steps = 6 , reward = 1.0\nEpisode 403: steps = 8 , reward = 1.0\nEpisode 404: steps = 6 , reward = 1.0\nEpisode 405: steps = 9 , reward = 1.0\nEpisode 406: steps = 8 , reward = 1.0\nEpisode 407: steps = 6 , reward = 1.0\nEpisode 408: steps = 8 , reward = 1.0\nEpisode 409: steps = 6 , reward = 1.0\nEpisode 410: steps = 6 , reward = 1.0\nEpisode 411: steps = 7 , reward = 1.0\nEpisode 412: steps = 5 , reward = 0.0\nEpisode 413: steps = 6 , reward = 1.0\nEpisode 414: steps = 6 , reward = 1.0\nEpisode 415: steps = 6 , reward = 1.0\nEpisode 416: steps = 8 , reward = 1.0\nEpisode 417: steps = 6 , reward = 1.0\nEpisode 418: steps = 6 , reward = 1.0\nEpisode 419: steps = 5 , reward = 0.0\nEpisode 420: steps = 6 , reward = 1.0\nEpisode 421: steps = 6 , reward = 1.0\nEpisode 422: steps = 7 , reward = 1.0\nEpisode 423: steps = 6 , reward = 1.0\nEpisode 424: steps = 6 , reward = 1.0\nEpisode 425: steps = 6 , reward = 1.0\nEpisode 426: steps = 6 , reward = 1.0\nEpisode 427: steps = 7 , reward = 1.0\nEpisode 428: steps = 6 , reward = 1.0\nEpisode 429: steps = 10 , reward = 1.0\nEpisode 430: steps = 4 , reward = 0.0\nEpisode 431: steps = 6 , reward = 1.0\nEpisode 432: steps = 7 , reward = 1.0\nEpisode 433: steps = 6 , reward = 0.0\nEpisode 434: steps = 7 , reward = 1.0\nEpisode 435: steps = 10 , reward = 1.0\nEpisode 436: steps = 5 , reward = 0.0\nEpisode 437: steps = 8 , reward = 1.0\nEpisode 438: steps = 9 , reward = 1.0\nEpisode 439: steps = 8 , reward = 1.0\nEpisode 440: steps = 6 , reward = 1.0\nEpisode 441: steps = 3 , reward = 0.0\nEpisode 442: steps = 6 , reward = 1.0\nEpisode 443: steps = 6 , reward = 1.0\nEpisode 444: steps = 6 , reward = 1.0\nEpisode 445: steps = 6 , reward = 1.0\nEpisode 446: steps = 8 , reward = 1.0\nEpisode 447: steps = 6 , reward = 1.0\nEpisode 448: steps = 6 , reward = 1.0\nEpisode 449: steps = 6 , reward = 1.0\nEpisode 450: steps = 6 , reward = 1.0\nEpisode 451: steps = 6 , reward = 1.0\nEpisode 452: steps = 6 , reward = 1.0\nEpisode 453: steps = 9 , reward = 1.0\nEpisode 454: steps = 6 , reward = 1.0\nEpisode 455: steps = 6 , reward = 1.0\nEpisode 456: steps = 6 , reward = 1.0\nEpisode 457: steps = 8 , reward = 1.0\nEpisode 458: steps = 6 , reward = 1.0\nEpisode 459: steps = 10 , reward = 1.0\nEpisode 460: steps = 7 , reward = 1.0\nEpisode 461: steps = 8 , reward = 1.0\nEpisode 462: steps = 6 , reward = 1.0\nEpisode 463: steps = 6 , reward = 1.0\nEpisode 464: steps = 8 , reward = 1.0\nEpisode 465: steps = 6 , reward = 1.0\nEpisode 466: steps = 8 , reward = 1.0\nEpisode 467: steps = 8 , reward = 1.0\nEpisode 468: steps = 6 , reward = 1.0\nEpisode 469: steps = 6 , reward = 1.0\nEpisode 470: steps = 6 , reward = 1.0\nEpisode 471: steps = 6 , reward = 1.0\nEpisode 472: steps = 6 , reward = 1.0\nEpisode 473: steps = 6 , reward = 1.0\nEpisode 474: steps = 6 , reward = 1.0\nEpisode 475: steps = 10 , reward = 1.0\nEpisode 476: steps = 8 , reward = 1.0\nEpisode 477: steps = 6 , reward = 1.0\nEpisode 478: steps = 8 , reward = 1.0\nEpisode 479: steps = 6 , reward = 1.0\nEpisode 480: steps = 8 , reward = 1.0\nEpisode 481: steps = 8 , reward = 1.0\nEpisode 482: steps = 6 , reward = 1.0\nEpisode 483: steps = 6 , reward = 1.0\nEpisode 484: steps = 6 , reward = 1.0\nEpisode 485: steps = 8 , reward = 1.0\nEpisode 486: steps = 6 , reward = 1.0\nEpisode 487: steps = 8 , reward = 1.0\nEpisode 488: steps = 6 , reward = 1.0\nEpisode 489: steps = 7 , reward = 1.0\nEpisode 490: steps = 6 , reward = 1.0\nEpisode 491: steps = 8 , reward = 1.0\nEpisode 492: steps = 8 , reward = 1.0\nEpisode 493: steps = 6 , reward = 1.0\nEpisode 494: steps = 8 , reward = 1.0\nEpisode 495: steps = 6 , reward = 1.0\nEpisode 496: steps = 6 , reward = 1.0\nEpisode 497: steps = 6 , reward = 1.0\nEpisode 498: steps = 6 , reward = 1.0\nEpisode 499: steps = 8 , reward = 1.0\ntest reward = 1.0\n"
    }
   ],
   "source": [
    "# 使用gym创建迷宫环境，设置is_slippery为False降低环境难度\n",
    "env = gym.make(\"FrozenLake-v0\", is_slippery=False)  # 0 left, 1 down, 2 right, 3 up\n",
    "\n",
    "# 创建一个agent实例，输入超参数\n",
    "agent = QLearningAgent(\n",
    "        obs_n=env.observation_space.n,\n",
    "        act_n=env.action_space.n,\n",
    "        learning_rate=0.1,\n",
    "        gamma=0.9,\n",
    "        e_greed=0.1)\n",
    "\n",
    "\n",
    "# 训练500个episode，打印每个episode的分数\n",
    "for episode in range(500):\n",
    "    ep_reward, ep_steps = run_episode(env, agent, False)\n",
    "    print('Episode %s: steps = %s , reward = %.1f' % (episode, ep_steps, ep_reward))\n",
    "\n",
    "# 全部训练结束，查看算法效果\n",
    "test_reward = test_episode(env, agent)\n",
    "print('test reward = %.1f' % (test_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit1f3025c149864e25af69bd18d8589ad3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}